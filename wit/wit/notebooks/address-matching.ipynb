{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning String Similarity for Address Resolution\n",
    "##### Ben Johnson, Phronesis LLC, 2015-12-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "\n",
    "In this notebook, we'll try to learn a custom string metric for address resolution.\n",
    "\n",
    "Generally, when we're doing this kind of thing, we use something like Levenshtein distance or Jaro-Winkler distance to measure the distance between two strings.  \n",
    "\n",
    "This generally makes sense for measuring distances between first names: \n",
    "\n",
    "    Hilary\n",
    "    Hillary\n",
    "    Hillry\n",
    "    \n",
    "are plausibly the same name with various typos.\n",
    "\n",
    "However, this is clearly not the best method for measuring distances between addresses:\n",
    "\n",
    "    123 Fake Street\n",
    "    123 Fake St\n",
    "    124 Fake Street\n",
    "\n",
    "From writing conventions, it's obvious that the first two addresses are the same and the third is different, despite the fact that the first and third have the smallest Levenshtein distance.  So we see that these kind of metrics are degraded when applied to addresses due to \n",
    "\n",
    "    different characters have different importance\n",
    "    abbreviations are commonly used\n",
    "\n",
    "#### Approach\n",
    "\n",
    "We could try to write down a set of weights for different characters and a mapping from words to abbreviations, but that would be tedious and may be brittle.  It also would require separate hand-written rules for each datatype.  We could spend time developing a set of rules for addresses, then realize that we need to do the same thing for corporation names, etc... So ideally we want to be able to learn the string similarity metric from data.\n",
    "\n",
    "(Note, in this document, we'll say two addresses are \"equivalent\" if they refer to the same physical location.)\n",
    "\n",
    "But what kind of training data can we use for this task?  It turns out that it is not impossible to use publicly available datasets to construct a reasonably high fidelity set of pairs of non-equivalent addresses and pairs of equivalent addresses.  The first set of pairs could be constructed by finding a high-quality list of unique addresses in a region and thus any pair of addresses is non-equivalent.  The second set is a little trickier, but could be constructed by taking a messy dataset and joining addresses on some less messy attribute (i.e. phone number).  This kind of dataset may be publicly available in the form of campaign finance records, government contracting records, etc...\n",
    "\n",
    "From these two sets of paired addresses, we can construct a training dataset consisting of triplets of addresses:\n",
    "\n",
    "    (anchor, positive, negative)\n",
    "\n",
    "where\n",
    "\n",
    "    - anchor and positive are equivalent\n",
    "    - anchor and negative are not equivalent\n",
    "\n",
    "We'll use this data to learn a function `F` that maps address strings to N-dimensional space while minimizing the function\n",
    "\n",
    "    max(0, distance(F(anchor), F(positive)) - distance(F(anchor), F(negative)) + margin)\n",
    "    \n",
    "over the training data, with\n",
    "\n",
    "    distance(x, y) = 1 - cosine_similarity(x, y)\n",
    "\n",
    "The model we use is a LSTM neural network with this triplet loss function implemented in the `Keras` `python` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load some libraries, set some parameters and initialize some helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/BenJohnson/projects/what-is-this/wit/')\n",
    "from wit import *\n",
    "\n",
    "# --\n",
    "# Setting parameters\n",
    "num_features = 75  # Dimension of character embedding (more than enough)\n",
    "max_len      = 350 # Max length of input string in characters\n",
    "formatter    = KerasFormatter(num_features, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data and construct a training set in the format we described above.\n",
    "\n",
    "(Note: At the moment, we won't actually create the negative training pairs using a high-quality list of unique addresses in a region.  Rather, we'll use randomly drawn pairs of addresses under the assumption that the number of addresses is sufficiently high as to make collisions relatively uncommon. We plan to re-run these analyses with higher quality negative training pairs in the near future.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --\n",
    "# Construct training set\n",
    "\n",
    "# Load set of matching addresses\n",
    "df         = pd.read_csv('real_address.csv')\n",
    "df['id']   = 0\n",
    "\n",
    "# Make training set\n",
    "train      = make_triplet_train(df.head(60000), N = 20)\n",
    "\n",
    "# Format training set for input into `wit`\n",
    "trn, levs  = formatter.format(train, ['obj'], 'hash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `wit` TripletClassifier model to learn the string embedding we described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = TripletClassifier(trn, levs)\n",
    "classifier.fit(batch_size = 250, nb_epoch = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can compare the behavior of the learned similarity metric to Levenshtein distance.  We expect Levenshtein distance to perform adequately on addresses that have different street names, but expect it to fare much worse on addresses with different (but similar) street numbers or different levels of abbreviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--\n",
      "Case 1, different street names : ('101 fake street', '101 elm street')\n",
      "\n",
      "wit_sim : 0.790000\n",
      "lev_sim : 83\n",
      "\n",
      "\n",
      "--\n",
      "Case 2, different abbrevations : ('101 fake street', '101 fake st')\n",
      "\n",
      "wit_sim : 0.959000\n",
      "lev_sim : 85\n",
      "\n",
      "\n",
      "--\n",
      "Case 3, different house numbers : ('101 fake street', '102 fake street')\n",
      "\n",
      "wit_sim : 0.861000\n",
      "lev_sim : 93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare(a, b):\n",
    "    # Similarity metric learned by `wit`\n",
    "    pred    = model.predict(formatter._format_x([a, b], False))\n",
    "    wit_sim = np.round(np.dot(pred, pred.T)[0, 1], 3)\n",
    "\n",
    "    # Fuzzywuzzy.fuzz.ratio Levenshtein similarity\n",
    "    fuzz_sim = int(np.round(fuzz.ratio(a, b)))\n",
    "    \n",
    "    print '\\nwit_sim : %f\\nlev_sim : %d\\n' %(wit_sim, fuzz_sim)\n",
    "\n",
    "print \"\\n--\\nCase 1, different street names : ('101 fake street', '101 elm street')\"\n",
    "compare('101 fake street', '101 elm street')\n",
    "\n",
    "print \"\\n--\\nCase 2, different abbrevations : ('101 fake street', '101 fake st')\"\n",
    "compare('101 fake street', '101 fake st')\n",
    "\n",
    "print \"\\n--\\nCase 3, different house numbers : ('101 fake street', '102 fake street')\"\n",
    "compare('101 fake street', '102 fake street')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic ranges of the two methods are different, but notice that the ordering of pairs is different between the two methods\n",
    "\n",
    "                         wit   lev\n",
    "    Most similar case     2     3 \n",
    "                          3     2\n",
    "    Least similar case    1     1\n",
    "\n",
    "`wit` seems to have learned the the varying importance of characters in the string, while Levenshtein continues to treat all characters equally.\n",
    "\n",
    "Further work on a robust comparison between the two methods is needed, ideally applied to a benchmarker de-duplication/record linkage dataset.  We would also like to construct reasonable training datasets for other types of strings where the imporance of characters is non-uniform (phone numbers, etc)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
